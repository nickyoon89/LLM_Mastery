{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "961xR8LdcJQY"
   },
   "outputs": [],
   "source": [
    "# Small LLM / Notebook created by Javier Ideami (ideami.com)\n",
    "# Typical LLMs need many GPUs and millions of dollars to be trained\n",
    "# This code trains a small LLM with a single GPU and little GPU memory \n",
    "# Of course results are not like a chatGPT, but they are good enough to see how the LLM trains to go\n",
    "# from random combinations of letters to actual words and phrases that are sometimes decently coherent\n",
    "# GPT3 has 175 Billion parameters. GPT4 has many, many more.\n",
    "# This model has only 19 Million parameters with its default settings. That's why its perfect for learning \n",
    "# and experimenting\n",
    "\n",
    "# Official notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1tUgAJccK6D",
    "outputId": "dbabcd5d-ad95-4518-a0f8-e04fddbce82c"
   },
   "outputs": [],
   "source": [
    "# uncomment and run the following installation lines ONLY if you havent installed these libraries already outside of the notebook\n",
    "#!pip install ipdb -q\n",
    "#!pip install tqdm -q\n",
    "#!pip install sentencepiece -q\n",
    "#!pip install wandb -q\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this command to view information about your GPU and the amount of free memory it has\n",
    "# Make sure that you have at last 4GB of free GPU memory to do this course\n",
    "!nvidia-smi \n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google colab, at the moment the option is within the Runtime menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VnNqwkhiU3n"
   },
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "\n",
    "import os, sys\n",
    "import ipdb # for debugging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil # detect platform type\n",
    "import requests, zipfile, io \n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sentencepiece as spm # For the tokenizer\n",
    "\n",
    "# These lines improve performance for Ampere Architecture (e.g: A100s)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# Empty GPU cache memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5q26l98govJ"
   },
   "outputs": [],
   "source": [
    "# Download necessary files and create necessary folders\n",
    "# wiki.txt - dataset: a tiny segment of the English Wikipedia\n",
    "# wiki_tokenizer.model: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# wiki_tokenizer.vocab: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# encoded_data.pt (dataset tokenized with the tokenizer)\n",
    "# I will explain how to produce encoded_data.pt - because it takes quite a bit to process, it's nice to have it in advance\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created. If you have any problems downloading the files with this code, I have also added llm_train.zip\n",
    "# to the downloadable resources of this lecture (however, best option is to use this code, because then you don't need\n",
    "# to upload the files or do anything else)\n",
    "\n",
    "files_url = \"https://ideami.com/llm_train\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"encoded_data.pt\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fre7fXD0fVD9",
    "outputId": "04d590af-d8cc-4e93-fd10-60b97fc473d1"
   },
   "outputs": [],
   "source": [
    "# Set main parameters\n",
    "\n",
    "# ARCHITECTURE PARAMETERS\n",
    "batch_size= 8 # How many samples do we train at once (set as needed, typical range 8 to 128)\n",
    "              # 8 is good for a GPU with 4GB of memory, 128 is good for a GPU with 24GB of memory\n",
    "context=512 # Sequence length used for training, 512 is a good compromise for our level of resources\n",
    "embed_size=384 # Embedding size\n",
    "n_layers = 7 # Number of transformer layers\n",
    "n_heads = 7 # Number of heads within each layer\n",
    "BIAS = True # Do we want Bias parameters?\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "lr = 3e-4 # Initial learning rate\n",
    "dropout=0.05 # Dropout percentage\n",
    "weight_decay = 0.01 # Weight decay regularizer\n",
    "grad_clip = 1.0 # Gradient clipping to prevent gradient explosion\n",
    "\n",
    "# TRAINING parameters\n",
    "train_iters = 100000 # Maximum number of training iterations\n",
    "eval_interval=50 # How often do we evaluate the performance?\n",
    "eval_iters=3 # Number of iterations while we evaluate performance\n",
    "compile = False # Compile will accelerate performance in compatible systems\n",
    "load_pretrained = False # Do we want to load a pretrained model to continue training?\n",
    "\n",
    "checkpoint_dir = 'models/'  # Where do we store checkpoints?\n",
    "\n",
    "checkpoint_fn = \"latest.pt\" \n",
    "# Name of checkpoint file to be saved during training\n",
    "\n",
    "checkpoint_load_fn = \"latest.pt\" \n",
    "# Name of checkpoint file to be loaded when load_pretrained is True\n",
    "# You can load llm2.pt to experiment with a checkpoint that already reached 2.31 of loss\n",
    "\n",
    "dtype = torch.bfloat16 # our target internal data type\n",
    "\n",
    "# MODE\n",
    "# Do we want to run the model in inference mode?\n",
    "inference=False \n",
    "\n",
    "# DEVICE - Sets device to GPU or CPU (use GPU always)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: You will be using: \",device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "0Z_omi-4fW0s",
    "outputId": "3f8ecc39-b72d-4825-a78a-2705f66a7210"
   },
   "outputs": [],
   "source": [
    "# LOGGING parameters\n",
    "# When you run this cell, it will ask you to enter your Wandb API Key, which you\n",
    "# can find at your account on https://wandb.ai/settings#api\n",
    "wandb_log = True\n",
    "wandb_project = \"test\"\n",
    "wandb_run_name = \"test-run\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "\n",
    "# The first time you run this logging code set to True, the weights and biases library\n",
    "# will ask you for an API key. You can follow the instructions in the video, or you can\n",
    "# also simply click on a link that should appear when you run this cell, pointing to this\n",
    "# address: https://wandb.ai/authorize  \n",
    "# Going to that address will allow you to quickly get an API key as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPrBqt7NhwnZ",
    "outputId": "85aad864-9cd1-4f74-fa76-63a24988a20f"
   },
   "outputs": [],
   "source": [
    "with open('wiki.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "\n",
    "print(text[10000:10500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDNcMXo_fals",
    "outputId": "8381a7a5-047a-425e-ece7-d958b18721e5"
   },
   "outputs": [],
   "source": [
    "# SENTENCEPIECE TOKENIZER\n",
    "\n",
    "# Load trained tokenizer\n",
    "# Make sure that \" model_file = \" is pointing to the right file\n",
    "sp = spm.SentencePieceProcessor(model_file='wiki_tokenizer.model')\n",
    "\n",
    "# Get the vocabulary size of our tokenizer\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")\n",
    "\n",
    "# Create the encoding and decoding tokenizer functions\n",
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "# Test that encoding and decoding are working well\n",
    "print(decode(encode(\"Encoding Decoding functions ready\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tA2mDSq_fhwC"
   },
   "outputs": [],
   "source": [
    "# Tokenization of the dataset\n",
    "if os.path.exists(f\"encoded_data.pt\"):\n",
    "    # Load encoded data if you already saved it previously\n",
    "    print(\"Loading saved encoded data\")\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    # If you still didn't encode and save the encoding, do it here\n",
    "    print(\"Encoding data\")\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data, 'encoded_data.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4B1cPQGnJM0",
    "outputId": "9b5d5e9d-db9a-4411-ef20-2177f99bf469"
   },
   "outputs": [],
   "source": [
    "data_size=len(data) # Get the size of the dataset\n",
    "\n",
    "spl = int(0.9*data_size) # set the split at 90%-10%\n",
    "train_data=data[:spl] # training data will be 90% of the dataset\n",
    "val_data=data[spl:] # validation data will be 10% of the dataset\n",
    "print(f'Total data: {data_size/1e6:.2f} Million | Training: {len(train_data)/1e6:.2f} Million | Validation: {len(val_data)/1e6:.2f} Million')\n",
    "\n",
    "# data[:30] : shows the first 30 token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EGi6Aevnjtp"
   },
   "outputs": [],
   "source": [
    "############## HELPER FUNCTIONS ###########################\n",
    "\n",
    "# Return a batch of either training or evaluation data\n",
    "def get_batch(split):\n",
    "    # BS = Batch Size / SL = Sequence Length or context length\n",
    "    data = train_data if split==\"train\" else val_data # Select the split\n",
    "    inds = torch.randint(len(data)-context, (batch_size,)) # (BS)\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (BS,SL)\n",
    "    y = torch.stack([data[i+1: i+context+1] for i in inds]) # (BS,SL)\n",
    "\n",
    "    # Examples of what it returns\n",
    "    # # First 10 elements of first batch of inputs and labels\n",
    "    #x[0][:10] -> tensor([ 664,  278, 4031, 4056, 4065, 4062, 4062, 4051, 13, 13])\n",
    "    #y[0][:10] -> tensor([ 278, 4031, 4056, 4065, 4062, 4062, 4051,   13, 13, 4066])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dusI66zcouBq",
    "outputId": "46d934e5-78c5-45a7-ea83-120ac72671ff"
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your get_batch function\n",
    "#x,y=get_batch(\"train\")\n",
    "#print(f\"x.shape: {x.shape}\")\n",
    "#print(f\"y.shape: {y.shape}\")\n",
    "#print(x[0][:10])\n",
    "#print(y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19G3Q_RKqVBd"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "################## LLM MODEL #############################################\n",
    "# 19 million parameters with the default configuration\n",
    "# Can be trained with 1 single GPU\n",
    "# With 8 Batch Size, should require 4 GB of GPU Memory\n",
    "# With 128 Batch Size, should require 24 GB of GPU Memory\n",
    "# Adjust Batch Size as needed for less or more memory and training speed\n",
    "# Because of small dataset and model, results will be limited but enough to\n",
    "# demonstrate good improvement during the training and understand all the\n",
    "# main technology involved in building LLMs\n",
    "#################################################################################\n",
    "###############################################\n",
    "##################################\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size,embed_size) # Create embedding layer\n",
    "        self.positions = nn.Embedding(context, embed_size) # Create basic positioning embeddings\n",
    "        self.blocks = nn.Sequential(*[Block(n_heads) for _ in range(n_layers)]) # setup transformer blocks\n",
    "        self.ln = nn.LayerNorm(embed_size) # normalization layers\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS) # feedforward linear layer\n",
    "        self.apply(self._init_weights) # Initialize the weights\n",
    "\n",
    "    # Weights initialization\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):        \n",
    "            # Initialize weight matrices with normal distribution with mean 0 and small std\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            # Initialize bias parameters to 0\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        # Initialize Embedding weights with normal distribution with mean 0 and small std\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # Running the LLM model\n",
    "    def forward(self, input, targets=None):\n",
    "        # BS = Batch Size / SL = Sequence Length or context length\n",
    "        # For easier reading, I assume embedding dim of 384 and vocab size of 4096 in comments\n",
    "        loss= None\n",
    "        BS,SL = input.shape  # (BS,SL)\n",
    "        emb = self.embeddings(input)  # (BS,SL,384)\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # (SL,384)\n",
    "        x = emb+pos  # combine embedding and positioning stages (BS,SL,384)\n",
    "        x = self.blocks(x)  #(BS,SL,384)\n",
    "        x = self.ln(x) # (BS,SL,384)\n",
    "        logits = self.final_linear(x) # (BS,SL,4096)\n",
    "\n",
    "        # Calculate Loss if training with targets\n",
    "\n",
    "        # Cross Entropy Logic\n",
    "        # (equivalent to negative log likelihood)\n",
    "\n",
    "        # Information: -log p(x) (inverse of probability)\n",
    "        # Entropy: avg of information in random variable (prob distribution): - sum_x (x * log(x))\n",
    "        # CrossEntropy: Compares 2 distr q(true) & p(predicted) in terms of information distance: -sum_x (q(x) * log p(x))\n",
    "        # LLMs CrossEntropy: true labels are 1 for true, 0 for the rest, so it simplifies to: -sum_x log p(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape  # (BS,SL,4096)\n",
    "            logits = logits.view(BS*SL,VS)  # Reshape to prepare for cross_entropy (BS*SL,4096)\n",
    "            targets = targets.view(BS*SL)   # Reshape as well (BS*SL)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "\n",
    "            # Optional: Just for fun, manual way to calculate cross_entropy\n",
    "            # By default, we comment out the manual version to prevent calculating the loss twice (will make things slower)\n",
    "\n",
    "            # First apply softmax to produce probabilities\n",
    "            #counts = logits.exp()  # (BS*SL,4096)\n",
    "            #prob = counts / counts.sum(-1, keepdim=True) # (BS*SL,4096),(BS*SL,1) = (BS*SL,4096)\n",
    "            #loss2 = -prob[torch.arange(BS*SL),targets].log().mean() # torch.arange(B*T) (BS*SL) | targets (BS*SL)\n",
    "\n",
    "            # Finally at each of prob's positions, we pick the index specified by the respective target\n",
    "            # example: targets[3]=329, prob[3][329] = 0.014\n",
    "\n",
    "            # Most times they will match, sometimes they will not because F.cross_entropy is more precise\n",
    "            # By uncommenting the following lines, you can see when they don't match \n",
    "            #if ( not torch.allclose(loss,loss2)):\n",
    "            #    print(f\"[Loss Diff] Pytorch:{loss.item()} Manual:{loss2.item()}\")\n",
    "\n",
    "        return logits,loss\n",
    "\n",
    "    # Generate a new sample\n",
    "    def generate(self, input, max=500):\n",
    "        # SL = Sequence Length or context length\n",
    "        for _ in range(max): # until you reach the maximum number of tokens\n",
    "            input = input[:,-context:] #(1, input length until max of SL)\n",
    "            logits, _ = self(input)  # (1, input length, 4096)\n",
    "            logits = logits[:,-1,:]  # Pick last probability discarding the dimension (1, 4096)\n",
    "            probs = F.softmax(logits, dim=-1) # (1,4096)\n",
    "            next = torch.multinomial(probs, num_samples=1) # Sample next token value\n",
    "            input = torch.cat((input,next),dim=1) # Add new token to the input\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXdTAGEWp-nz"
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "##########Transformer Block Class ######\n",
    "########################################\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # A transformer block combines communication and computation over the data\n",
    "    # Helps create complex processing and also emphasize relationships between the\n",
    "    # members of the sequence through the attention mechanisms\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads # We split the embedding dimensions among the number of heads\n",
    "        self.ma = Multihead(n_heads,head_size) # We setup the multihead system within each block\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size) # Normalizing layer\n",
    "        self.ln2 = nn.LayerNorm(embed_size) # Normalizing layer\n",
    "\n",
    "        # LayerNorm normalizes the inputs across the features for each data point independently.\n",
    "        # It subtracts the mean and divides by the standard deviation, followed by scaling and shifting.\n",
    "        # It is computationally more intensive than for example RMSnorm but offers greater flexibility.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.ma(self.ln1(x))  # We normalize and then apply multi head attention\n",
    "        x = x + self.feed_forward(self.ln2(x)) # we normalize again and then apply a feed forward layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPT2Akr9tB-c"
   },
   "outputs": [],
   "source": [
    "# The ForwardLayer applies a network that increases the computational complexity of the processing \n",
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size, 6*embed_size, bias=BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embed_size, embed_size, bias=BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNdQG5IotEtj"
   },
   "outputs": [],
   "source": [
    "# Multihead Attention Layer\n",
    "# This layer coordinates the different attention heads within each transformer block\n",
    "class Multihead(nn.Module):\n",
    "    def __init__(self,n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)]) # Setup the heads | head_size = embed_size // n_heads\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias=BIAS) # (378,384) - in case of our default values\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BS = Batch Size / SL = Sequence Length or context length\n",
    "        # x is (BS,SL,384)  # 384 is default embed size\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # Each head outputs (BS,SL, head_size)\n",
    "        # Combining them with torch.cat produces (BS,SL,378)  378 is default head_size * default n_heads = 54 * 7\n",
    "        x = self.combine(x) # project them back to embed_size (BS, SL, 384)  384 is default embed_size\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34nHc2eJtH17"
   },
   "outputs": [],
   "source": [
    "# Head Attention Layer\n",
    "# Detects and reinforces patterns in relationships between members of sequence\n",
    "class Head(nn.Module):\n",
    "    # BS = Batch Size / SL = Sequence Length or context length\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries= nn.Linear(embed_size, head_size, bias=BIAS) # Query Projection (embed_dim, head_size) (384, 54)\n",
    "        self.keys= nn.Linear(embed_size, head_size, bias=BIAS) # Key Projection (384, 54)\n",
    "        self.values= nn.Linear(embed_size, head_size, bias=BIAS) # Value Projection (384, 54)\n",
    "        # We declare a triangular matrix that we will use to mask future tokens from the current position\n",
    "        # self.tril contains 0s in upper triangle and 1s in lower triangle + diagonal\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(context,context))) # self.tril - (SL,SL)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        BS,SL, VS = x.shape\n",
    "        q=self.queries(x) # (BS,SL,54)  54 is the head_size\n",
    "        k=self.keys(x) # (BS,SL,54)\n",
    "        v=self.values(x) # (BS,SL,54)\n",
    "\n",
    "        # Calculate square attention weights matrix with dot product of q and k, and normalize\n",
    "        attn_w = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (BS, SL, SL)\n",
    "\n",
    "        # mask out future tokens, pay attention only to the past\n",
    "        attn_w = attn_w.masked_fill(self.tril[:SL,:SL]==0, float('-inf'))  # set to -inf the upper right triangle of 0s\n",
    "\n",
    "        attn_w = F.softmax(attn_w, dim=-1) # Transform into probabilities (BS, SL, SL)\n",
    "        attn_w = self.dropout(attn_w) # (BS, SL, SL)\n",
    "\n",
    "        # use attention weights to update the features of our tokens\n",
    "        x = attn_w @ v # (BS,SL,54) # 54 is the head_size = embed_dim // n_heads\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKRu7PKctLIS",
    "outputId": "2b6861cf-fc98-4a3b-c91c-8233cb613d79"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Main Training Process\n",
    "#################################################################################\n",
    "\n",
    "# Main Setup\n",
    "\n",
    "model = GPT() # Instantiate LLM\n",
    "model = model.to(dtype) # Set the precision type\n",
    "model = model.to(device) # Move it to the right device\n",
    "\n",
    "# Torch.compile compiles a PyTorch model to an optimized version, aiming to improve runtime performance and efficiency.\n",
    "# Disable if your system doesn't support it\n",
    "if compile:\n",
    "    print(\"Torch :: Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "# Print the number of parameters of our model (19 million in our case)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \" Million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvX0LI8HtR_h"
   },
   "outputs": [],
   "source": [
    "# Calculate the Loss\n",
    "@torch.no_grad()  # Prevent gradient calculation\n",
    "def calculate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','eval']:        \n",
    "        l=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        for i in range(eval_iters):\n",
    "            x,y=get_batch(split) # Get a new batch of data\n",
    "            _,loss=model(x,y)  # Calculate the loss\n",
    "            l[i]=loss  # Store the loss in the next position of tensor\n",
    "        out[split]=l.mean().item()  # Calculate the mean and extract the final value\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "l=calculate_loss()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X20ZTtOu9mJ",
    "outputId": "bcf6bf71-bf8a-4412-c9b0-283d528943d7"
   },
   "outputs": [],
   "source": [
    "# Generate a new sample\n",
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device) # Tokenize string -> (tensor of ids)\n",
    "    t1 = t1[None,:]  # (1 , [size of ids])\n",
    "    newgen = model.generate(t1,max=64)[0].tolist() # call the generate method, limit output size\n",
    "    result=decode(newgen) # decode the result with the tokenizer to get back characters\n",
    "    print(f\"{result}\")\n",
    "\n",
    "generate_sample(\"The mountain in my city is\") # Generate a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z9sOljjvq2l"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Main Training Process\n",
    "#################################################################################\n",
    "\n",
    "# Set Weight Decay differently for different kinds of parameters\n",
    "# parameter dictionary where keys are parameter names, and values are the parameter themselves\n",
    "p_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad} # len: 370\n",
    "\n",
    "# isolate weight matrices as they benefit specially from weight decay\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]  # len: 171\n",
    "\n",
    "# isolate other parameters like bias parameters, that don't benefit from weight decay\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2] # len: 199\n",
    "\n",
    "# store the parameter types in a list of dictionaries\n",
    "optimizer_groups = [\n",
    "    {'params': weight_decay_p, 'weight_decay': weight_decay},\n",
    "    {'params': no_weight_decay_p, 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Declare optimizer, it helps us compute gradients, update parameters, manage learning rate, apply weight decay\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=lr, betas=(0.9, 0.99))\n",
    "# betas: control the exponential moving averages of the gradient and its square,\n",
    "# which are essential components of the Adam and AdamW optimization algorithms.\n",
    "\n",
    "# Declare scheduler to change learning rate through the training\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "# learning rate will descend till a minimum of a tenth of the lr\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float('inf')  # Track best loss value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXt7xGMvwQ_5"
   },
   "outputs": [],
   "source": [
    "# Loading Checkpoints\n",
    "\n",
    "# Loads a previously saved checkpoint\n",
    "def load_checkpoint(path):\n",
    "    print(\"LLM - Loading model\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) # Load parameters\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # Load optimizer state\n",
    "    iteration = checkpoint['iteration'] # In what iteration did we save the model?\n",
    "    loss = checkpoint['loss'] # What was the last loss value?\n",
    "    print(f\"Loaded iter {iteration} with loss {loss}\")\n",
    "    return iteration, loss\n",
    "\n",
    "################# OPTIONAL : LOAD A PREVIOUS CHECKPOINT\n",
    "if os.path.exists(f\"{checkpoint_dir}/{checkpoint_load_fn}\") and load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)\n",
    "    best_val_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2KPyh1cwo3t"
   },
   "outputs": [],
   "source": [
    "#### INFERENCE MODE - Activate inference and then exit\n",
    "if inference==True:\n",
    "    model.eval()\n",
    "    while True:\n",
    "         qs = input(\"Enter text (q to quit) >>> \")\n",
    "         if qs == \"\":\n",
    "             continue\n",
    "         if qs == 'q':\n",
    "             break\n",
    "         generate_sample(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "2zUtal8zwsUl",
    "outputId": "435ac1e0-8bb5-45f6-d17a-8f81e4e444d1"
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "###################### TRAINING #################################\n",
    "#################################################################\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(start_iteration, train_iters)):\n",
    "        xb,yb = get_batch(\"train\") # Get a new batch of data\n",
    "        logits,loss = model(xb,yb) # Run the LLM and get the logits and the loss\n",
    "\n",
    "        if (i % eval_interval==0 or i == train_iters-1): # Calculate the loss\n",
    "            l = calculate_loss()\n",
    "            print(f\"\\n{i}: train loss: {l['train']} / val loss: {l['eval']}\")\n",
    "\n",
    "            # We do a quick test so that we observe the evolution through the training\n",
    "            # Remember that we use a very small dataset which doesn't include all topics\n",
    "            generate_sample(\"The mountain in my city is\") # Generate a sample\n",
    "\n",
    "            if l['eval'] < best_val_loss: # If we improved the best loss, save a checkpoint\n",
    "                best_val_loss = l['eval']\n",
    "                print(\"[CHECKPOINT]: Saving with loss: \", best_val_loss)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_val_loss,\n",
    "                    'iteration': i,\n",
    "                }, checkpoint_dir + checkpoint_fn)\n",
    "\n",
    "            if wandb_log:\n",
    "                wandb.log({\n",
    "                        \"loss/train\": l['train'],\n",
    "                        \"loss/val\": l['eval'],\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                    },\n",
    "                    step = i)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True) # Reset gradients\n",
    "        loss.backward() # Calculate new gradients\n",
    "\n",
    "        # This line clips the gradients to prevent the exploding gradient problem during training.\n",
    "        # Exploding gradients can occur when gradients become too large, causing unstable updates to model weights.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "        optimizer.step() # Update the model parameters\n",
    "        scheduler.step() # Update the learning rate value\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory released.\")\n",
    "\n",
    "if wandb_log:   \n",
    "    wandb.finish()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Code designed by Javier ideami\n",
    "# ideami.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnYRMN1-xAtK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
