{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af875c-22fd-4246-a5e5-3d72844a7b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files\n",
    "\n",
    "import requests, zipfile, io\n",
    "\n",
    "files_url = \"https://ideami.com/llm_align\"\n",
    "print(\"Downloading files using Python\")\n",
    "# response = requests.get(files_url)\n",
    "# zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d94c0-ca31-436f-927e-6e9ab647366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, sys\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import ipdb\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Import some HuggingFace Libraries\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# Performance (if you have cuda)\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Optional, for debugging, if you want to view entire tensors\n",
    "torch.set_printoptions(threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14152d-a859-4e26-b061-a367dd0be595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 1\n",
    "epochs = 3 # after 3 epochs can possibly degrade or get worse\n",
    "lr = 6e-5\n",
    "lr_warmup_steps = 100 # increase learning rate until 100 steps\n",
    "context = 1024\n",
    "alpha = 0.5 # weighting for the ORPO odds ratio (sort of loss calculation variable)\n",
    "prompt_max_size = 512 # limit for the prompt part of the interaction. \n",
    "# prompt: includes all the interaction except the last answer\n",
    "# response: includes either the positive chosen answer or the negative rejected one\n",
    "compile = False\n",
    "dtype = torch.bfloat16\n",
    "log_iters = 50\n",
    "\n",
    "#HYPERPARAMETERS\n",
    "dropout = 0.\n",
    "grad_clip = 1.0\n",
    "weight_decay = 0.0\n",
    "\n",
    "# DEVICE\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" #iOS: mps, Windows: cpu\n",
    "print(\"device: You will be using: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07e5b9-54de-4d6f-bc8e-2f1623f3e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGING\n",
    "project_name = \"alignment\"\n",
    "wandb_log = True\n",
    "wandb_project = project_name\n",
    "# wandb_run_name = \"aligntest-run\"\n",
    "wandb_run_name = \"aligntest-run\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") # recommended\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eff7f7-4a2d-4d57-90e4-daa82fdbe62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./data/orpo_dataset6\"\n",
    "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "tokenizer_path = \"tokenizers/tok16384\"\n",
    "checkpoint_dir = \"./models/\"\n",
    "\n",
    "# Tokenizing Dataset\n",
    "# Load tokenizer in HuggingFace Format\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Set our interaction template\n",
    "# tokenizer.chat_template = \"{% for message in messages %}{% if message['role']=='user' %}\\n{{ '<|user|> + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>' + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{%endif %}\\n{% endfor %}\"\n",
    "tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "{% if message['role'] == 'user' %}\n",
    "<|user|>{{ message['content'] }}{{ eos_token }}\n",
    "{% elif message['role'] == 'assistant' %}\n",
    "<|assistant|>{{ message['content'] }}{{ eos_token }}\n",
    "{% endif %}\n",
    "{% if loop.last and add_generation_prompt %}\n",
    "<|assistant|>\n",
    "{% endif %}\n",
    "{% endfor %}\"\"\"\n",
    "\n",
    "# Make padding token equal to the end of sentence token (which has ID of 2 in our case)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "else:\n",
    "    print(\"Filtering and tokenizing datset\")\n",
    "    dataset = load_dataset(dataset_name, split=\"all\")\n",
    "    # Now we will tokenize it\n",
    "\n",
    "    # Optional: Filter some of the entries # 37136 -> 36622\n",
    "    dataset = dataset.filter(lambda r: r[\"source\"] != \"toxic-dpo-v0.2\")\n",
    "\n",
    "    # FILTER DATASET\n",
    "    # Eliminate entries longer than 512(prompt_max_size). \n",
    "    # this is important because we want the prompt + answer to fit within the total context (1024)\n",
    "    def filter_dataset(examples):\n",
    "        prompt_length = tokenizer.apply_chat_template(examples['chosen'][:-1], tokenize=True, add_generation_prompt=True, return_tensors='pt').size(-1)\n",
    "\n",
    "        if prompt_length < prompt_max_size: #512\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Preprcess and tokenize data\n",
    "    def preprocess_dataset(examples: Union[List,Dict]):\n",
    "        # Take chosen field, eleminate last answer, apply chat template adding assistant prompt\n",
    "        prompt = [tokenizer.apply_chat_template(item[:-1], tokenize=False, add_generation_prompt=True) for item in examples['chosen']]\n",
    "        chosen = [tokenizer.apply_chat_template(item, tokenize=False) for item in examples['chosen']]\n",
    "        rejected = [tokenizer.apply_chat_template(item, tokenize=False) for item in examples['rejected']]\n",
    "\n",
    "        # Tokenize\n",
    "        # HF Tokenizer Dict Format\n",
    "        # Fields: ids, type_ids, tokens, offsets, attention_mask, special_token_mask, overflowing\n",
    "        inputs = tokenizer(prompt, max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        # debug: inputs.input_ids[0], inputs.attention_maks[0]\n",
    "        pos_labels = tokenizer(chosen, max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        neg_labels = tokenizer(rejected, max_length=context, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "        inputs['positive_input_ids'] = pos_labels['input_ids']\n",
    "        inputs['positive_attention_mask'] = pos_labels['attention_mask']\n",
    "        \n",
    "        inputs['negative_input_ids'] = neg_labels['input_ids']\n",
    "        inputs['negative_attention_mask'] = neg_labels['attention_mask']\n",
    "\n",
    "        # Prompt: inputs['input_ids'][0], inputs['attention_mask'][0]\n",
    "        # Positive: inputs['positive_input_ids'][0], inputs['positive_attention_mask'][0]\n",
    "        # Negative: inputs['negative_input_ids'][0], inputs['negative_attention_mask'][0]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    # Excluding prompts that are too long\n",
    "    dataset = dataset.filter(filter_dataset)\n",
    "\n",
    "    # Preprocess and tokenize dataset\n",
    "    # If you have issues with multiprocessing, change num_proc = 1\n",
    "    # For multiprocessing: num_proc=min(32, os.cpu_count())\n",
    "    # by default sending batches of 1000\n",
    "    dataset = dataset.map(preprocess_dataset, batched=True, num_proc=1, remove_columns=dataset.column_names)\n",
    "\n",
    "    dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8f4b4-0251-41c2-b5b5-8f57ffc2d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['positive_input_ids']\n",
    "tokenizer.decode(dataset[0]['positive_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08290e6b-7847-46d8-8e65-3255761898bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Split the data\n",
    "if isinstance(dataset, Dataset):\n",
    "    dataset = dataset.shuffle(42).train_test_split(test_size=0.05)\n",
    "elif isinstance(dataset, DatasetDict):\n",
    "    print(\"It is already a DatasetDict, skipping split.\")\n",
    "train_data = dataset['train']\n",
    "# features: 'input_ids', 'attention_mask'\n",
    "val_data = dataset['test']\n",
    "# features: 'input_ids', 'attention_mask'\n",
    "\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Setup DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, collate_fn=data_collator, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=data_collator, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6af21-827e-478e-a2f1-377003f5cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)\n",
    "batch = next(it)\n",
    "#print(tokenizer.decode(batch['positive_input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327c5ac-56b4-4357-8bfe-8c54aa6111d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ARCHITECTURE\n",
    "\n",
    "from llm import Llama, ModelArgs\n",
    "\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, \"base_model.pt\"), weights_only=False)\n",
    "config = checkpoint.pop(\"config\")\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    dim=config.hidden_size,\n",
    "    n_layers=config.num_hidden_layers,\n",
    "    n_heads=config.num_attention_heads,\n",
    "    n_kv_heads=config.num_key_value_heads,\n",
    "    vocab_size=config.vocab_size,\n",
    "    norm_eps=config.rms_norm_eps,\n",
    "    rope_theta=config.rope_theta,\n",
    "    max_seq_len=context,\n",
    "    dropout=config.attention_dropout,\n",
    "    hidden_dim=config.intermediate_size,\n",
    "    attention_bias=config.attention_bias,\n",
    "    mlp_bias=config.mlp_bias\n",
    ")\n",
    "# dim=768, n_layers=12, n_heads=12, vocab=16384, etc\n",
    "\n",
    "model = Llama(model_args)\n",
    "model.load_state_dict(checkpoint)\n",
    "model=model.to(dtype)\n",
    "model=model.to(device)\n",
    "model.train()\n",
    "\n",
    "if compile:\n",
    "    print(\"[INFO] Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \" Million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7503adb7-7d42-47ba-be0b-a15e45130cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), eps=1e-8, fused =device =='mps', weight_decay=weight_decay)\n",
    "# cuda, if it's Windows/Linux and using GPU not \"mps\". currently using it because the code is run in iOS\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "print(f\"num_training_steps: {num_training_steps}\")\n",
    "\n",
    "# Scheduler for lr: first 100 steps, we do a warmup in which we increase linearly the lr\n",
    "# After warmup, we decrease it gradually following a cosine curve\n",
    "\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < lr_warmup_steps:\n",
    "        return float(current_step)/float(max(1, lr_warmup_steps))\n",
    "    progress = float(current_step - lr_warmup_steps) / float(max(1, num_training_steps-lr_warmup_steps))\n",
    "    return max(0.0, 0.5*(1.0 + math.cos(math.pi*float(0.5) * 2.0  * progress)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9465db2-c69c-4987-a32c-ca2c3cf9f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logps(prompt_attention_mask, chosen_inputs, chosen_attention_mask, logits):\n",
    "    # conpute the PER TOKEN Log Probabilities\n",
    "\n",
    "    mask = chosen_attention_mask[:,:-1] - prompt_attention_mask[:,1:]\n",
    "    per_token_logps = torch.gather(logits[:,:-1,:].log_softmax(-1), dim=2, index=(mask * chosen_inputs[:, 1:]).unsqueeze(2)).squeeze(2)\n",
    "\n",
    "    return torch.mul(per_token_logps, mask.to(dtype)).sum(dim=1).to(dtype)/mask.sum(dim=1).to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eca3d2-730c-4ce2-a4a2-bc412bcbf6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOY EXAMPLE TO UNDERSTAND THE COMPUTATION\n",
    "p_mask = torch.tensor([1,1,1,1,0,0,0,0,0,0,0]) # 1s are the prompt\n",
    "c_mask = torch.tensor([1,1,1,1,1,1,1,0,0,0,0]) # answer is three 1s\n",
    "c_inputs = torch.tensor([2,4,3,2,4,2,1,0,0,0,0])\n",
    "print(p_mask)\n",
    "print(c_mask)\n",
    "print(c_inputs)\n",
    "\n",
    "mask = c_mask[:-1] - p_mask[1:]\n",
    "print(mask)\n",
    "\n",
    "logits = torch.tensor([\n",
    "    [0.1583, 0.0794, 0.1967, 0.3643, 0.2013],\n",
    "    [0.2517, 0.0463, 0.2702, 0.1700, 0.2617],\n",
    "    [0.1405, 0.1943, 0.1371, 0.1557, 0.3724],\n",
    "    [0.1266, 0.2257, 0.2330, 0.1872, 0.2275],\n",
    "    [0.1685, 0.2091, 0.1680, 0.1649, 0.2895],\n",
    "    [0.1993, 0.1869, 0.2283, 0.2176, 0.1679],\n",
    "    [0.1359, 0.2634, 0.1817, 0.1952, 0.2237],\n",
    "    [0.2431, 0.1394, 0.1615, 0.2876, 0.1684],\n",
    "    [0.2019, 0.2145, 0.2046, 0.1186, 0.2604],\n",
    "    [0.2340, 0.1782, 0.2505, 0.1342, 0.2031],\n",
    "    [0.1667, 0.2784, 0.1309, 0.1384, 0.2856],\n",
    "])\n",
    "\n",
    "print(c_inputs[1:]) # to match the last token of prompt to the first token of answer\n",
    "index=(mask * c_inputs[1:])\n",
    "print(index)\n",
    "print(index.shape)\n",
    "print(logits.shape)\n",
    "\n",
    "# Expand dimensions for correct gather shape\n",
    "index_expanded = index.unsqueeze(1)\n",
    "print(index_expanded)\n",
    "print(index_expanded.shape)\n",
    "\n",
    "# Gather the values at the specified indicies\n",
    "gathered_values = torch.gather(logits[:-1,:], dim=1, index=index_expanded) # get rid of the last logits because there is nothing to predict\n",
    "print(gathered_values)\n",
    "\n",
    "# Squeeze to remove the unnecessary dimension\n",
    "per_token_logps = gathered_values.squeeze(1)\n",
    "print(per_token_logps)\n",
    "\n",
    "result = torch.mul(per_token_logps, mask)\n",
    "print(result)\n",
    "f1 = result.sum(dim=0)\n",
    "f2 = mask.sum(dim=0)\n",
    "print(f1)\n",
    "print(f2)\n",
    "final = f1/f2\n",
    "print(final)\n",
    "# torch.gather(logits[:, :-1, :].log_softmax(-1), dim=2, index=(mask * chosen_inputs[:, 1:])\n",
    "                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74771057-fdee-4e60-a4c4-861b735c9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIGNMENT TRAINING LOOP\n",
    "\n",
    "try:\n",
    "    for e in range(epochs):\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader), dynamic_ncols=True):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n",
    "            batch[\"positive_input_ids\"] = batch[\"positive_input_ids\"].to(device)\n",
    "            batch[\"positive_attention_mask\"] = batch[\"positive_attention_mask\"].to(device)\n",
    "            batch[\"negative_input_ids\"] = batch[\"negative_input_ids\"].to(device)\n",
    "            batch[\"negative_attention_mask\"] = batch[\"negative_attention_mask\"].to(device)\n",
    "            batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "            neg_labels = batch[\"negative_input_ids\"].clone()\n",
    "            pos_labels = batch[\"positive_input_ids\"].clone()\n",
    "\n",
    "\n",
    "            # CALCULATING THE LOSS\n",
    "            mask = batch[\"attention_mask\"] * batch[\"positive_attention_mask\"]\n",
    "            # mask = batch['attention_mask'] will be the same since the attention is 1/0\n",
    "            pos_labels = pos_labels * mask.logical_not()\n",
    "            # put 0s where the prompt was, preserve the last answer, padding tokens have eos (2)\n",
    "\n",
    "            pos_labels[pos_labels == 0] = tokenizer.pad_token_id # eos: 2\n",
    "            pos_labels[pos_labels == tokenizer.pad_token_id] = - 100\n",
    "            neg_labels[neg_labels == tokenizer.pad_token_id] = - 100\n",
    "\n",
    "            outputs_pos, loss_pos = model(batch[\"positive_input_ids\"], pos_labels)\n",
    "            outputs_neg, loss_neg = model(batch[\"negative_input_ids\"], neg_labels)\n",
    "\n",
    "            # Calculate per token Log probabilities, essential to calculate the ORPO LOG ODDS RATIO\n",
    "\n",
    "            pos_prob = compute_logps(\n",
    "                prompt_attention_mask=batch['attention_mask'],\n",
    "                chosen_inputs = batch[\"positive_input_ids\"],\n",
    "                chosen_attention_mask = batch['positive_attention_mask'],\n",
    "                logits = outputs_pos\n",
    "            )\n",
    "            neg_prob = compute_logps(\n",
    "                prompt_attention_mask=batch['attention_mask'],\n",
    "                chosen_inputs = batch[\"negative_input_ids\"],\n",
    "                chosen_attention_mask = batch['negative_attention_mask'],\n",
    "                logits = outputs_neg\n",
    "            )\n",
    "\n",
    "            # Calcualte Orpo Odds Ratio\n",
    "            log_odds = (pos_prob - neg_prob) - torch.log(1-torch.exp(pos_prob)) - torch.log(1-torch.exp(neg_prob))\n",
    "            sig_ratio = F.sigmoid(log_odds)\n",
    "            ratio = torch.log(sig_ratio)\n",
    "\n",
    "            # Calculate the final loss\n",
    "            loss = torch.mean(loss_pos - (alpha*ratio).mean()).to(dtype=dtype)\n",
    "\n",
    "            # Logging\n",
    "            if i%log_iters == 0:\n",
    "                print(f\"Epochs [{e}/{epochs}] Step: [{i}/{len(train_loader)}], train loss: {loss.item():.3f}, Odds Ratio: {log_odds.mean().item():.3f}\")\n",
    "\n",
    "                if wandb_log:\n",
    "                    wandb.log({\n",
    "                        \"train_loss\":loss.item(),\n",
    "                        \"log_odds\":log_odds.mean().item(),\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                    },\n",
    "                    step = (e*len(train_loader)+i))\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    if wandb_log:\n",
    "                        wandb.finish()\n",
    "                    # torch.cuda.empty_cache()\n",
    "                    sys.exit()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        sd = model.state_dict()\n",
    "        sd['config'] = config\n",
    "        torch.save(sd, os.path.join(checkpoint_dir, f'{project_name}_{e+1}.pt'))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up..\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU Memory\n",
    "    # torch.cuda.empty_cache()\n",
    "    print(\"GPU memory released\")\n",
    "    # sys.exit(0)\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.finish()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
