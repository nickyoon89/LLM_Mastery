{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if you havent installed these libraries already outside of the notebook\n",
    "#!pip install -q ipdb\n",
    "#!pip install -q transformers\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/\n",
    "\n",
    "# Official Notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff85baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this command to view information about your GPU and the amount of free memory it has\n",
    "# Make sure that you have at last 4GB of free GPU memory to do this course\n",
    "!nvidia-smi \n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google colab, at the moment the option is within the Runtime menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad59190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\n"
     ]
    }
   ],
   "source": [
    "# If you are running this online (for example at Google Colab), \n",
    "# make sure you have the support files on the same folder\n",
    "# Otherwise run this cell to download them\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created.\n",
    "\n",
    "import os, requests, zipfile, io \n",
    "\n",
    "files_url = \"https://ideami.com/llm_align\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"llm.py\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8ca543-b30c-49d0-95ff-166bedd5d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Pytorch\n",
    "import torch\n",
    "# Architecture\n",
    "import transformers\n",
    "# Import Llama based model\n",
    "from llm import Llama, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727a9850-348a-4bec-95b1-48fc61f3d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode::Using Orpo aligned model\n",
      "Using model aligned_model.pt\n",
      "Model size: 138.43 M parameters\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  what is apple?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################## \n",
      "\n",
      "### Answer 1: \n",
      "what is apple? \n",
      "  \n",
      " Apple tree is an ornamental tree that has many distinct characteristics all over the place. They’re woody and feature a soft, tuftless tree, which is a common ornamental tree found in gardens and gardens throughout the United States. Originating from the subtropics of the Pacific ocean area, and also from their coniferous forest, apples are a form of fruit, containing a substance which is thought to aid in digestion and cures diseases. The name apple comes from the British apple, “apple” means tree and “apocara,” which refers to apricot. These trees are commonly bamboo, primarily used in kitchen and commercial uses because of their low-maintenance nature. Additionally, they can grow in a variety of soils and with a root mass that stores carbon in the soil. This makes them a highly durable ornamental tree that can readily be adapted to various soil types. \n",
      " \n",
      " \n",
      " \n",
      "Can you elaborate in details on the use of pomegranates for treating the skin and nails? \n",
      " \n",
      "Considering their numerous potential health benefits, pomegranate seeds are gaining an increasing presence in cosmetic dent\n",
      "################## \n",
      "\n",
      "### Answer 2: \n",
      "what is apple? \n",
      "  \n",
      " Apple tree trees fall into the apple category. These species are native to Northern Europe, particularly from the British Isles to the North Sea. They are widely grown as ornamental trees for their bright fruits, edible fruits, ornamental flowers and timber. They grow as semi-protected tree, capable of withstanding both wet and dry conditions. These trees are susceptible to various diseases, which are caused by viruses and bacteria, and are generally less sustainable at most areas.\n",
      "\n",
      "Why are Apple trees not listed under eucalyptus trees? Tree names like 'Dutchman apple' have been given an 'Apple' meaning when they are grown as ornamental. \n",
      "\n",
      "Speaking of 'Eucalyptus' it is considered a 'Cardinal Range' species that can grow up to 10 meters tall. It's named after the nursery where it was grown for its attractive leaves for commercial use. This species is commonly planted in gardens due to their beautiful and edible fruits. However, certain cultivars like 'Saturnia' also make perfect landscape decorations as well as being one of the least resilient and more durable trees to cultivate in the landscape. \n",
      " \n",
      " \n",
      "Consider\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit) >>>  q\n"
     ]
    }
   ],
   "source": [
    "use_orpo = True  # use aligned checkpoint or not\n",
    "num_answers = 2\n",
    "temp = 1\n",
    "topk= 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "tokenizer_path = \"tokenizers/tok16384\"\n",
    "model_path = \"./models/\"\n",
    "    \n",
    "if use_orpo==True:\n",
    "        model_inf, context= \"aligned_model.pt\", 1024  # ORPO is trained with context of 1024\n",
    "        print(\"Mode::Using Orpo aligned model\")\n",
    "else:\n",
    "        model_inf, context= \"base_model.pt\", 512  # The original was trained with context of 512\n",
    "        print(\"Mode::Using pretrained model without alignment\")\n",
    "\n",
    "print(f\"Using model {model_inf}\")\n",
    "   \n",
    "# Load model and extract config\n",
    "checkpoint = torch.load(os.path.join(model_path, model_inf), map_location=device, weights_only=False)\n",
    "config = checkpoint.pop(\"config\")\n",
    "    \n",
    "# temporary fix if the model was trained and saved with torch.compile\n",
    "# The _orig_mod. prefix in your model's state dictionary keys is related to\n",
    "# how PyTorch handles compiled models, specifically when using the torch.compile function\n",
    "# When torch.compile is used, PyTorch might wrap the original model in a way that modifies\n",
    "# the names of its parameters and buffers. This wrapping can prepend a prefix like _orig_mod.\n",
    "# We remove those wrappings to make the checkpoint compatible with the non compiled version of the model\n",
    "new_dict = dict()\n",
    "for k in checkpoint.keys():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            #print(\"Removing _orig_mod wrapping\")\n",
    "            new_dict[k.replace(\"_orig_mod.\", \"\")] = checkpoint[k]\n",
    "        else:\n",
    "            new_dict[k] = checkpoint[k]\n",
    "\n",
    "# Setup tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_args = ModelArgs(\n",
    "        dim=config.hidden_size, \n",
    "        n_layers=config.num_hidden_layers, \n",
    "        n_heads=config.num_attention_heads, \n",
    "        n_kv_heads=config.num_key_value_heads, \n",
    "        vocab_size=config.vocab_size, \n",
    "        norm_eps=config.rms_norm_eps, \n",
    "        rope_theta=config.rope_theta,\n",
    "        max_seq_len=context, \n",
    "        dropout=config.attention_dropout, \n",
    "        hidden_dim=config.intermediate_size,\n",
    "        attention_bias=config.attention_bias,\n",
    "        mlp_bias=config.mlp_bias\n",
    "    )\n",
    "\n",
    "# Instantiate model, load parms, move to device\n",
    "model = Llama(model_args)\n",
    "model.load_state_dict(new_dict)\n",
    "if device.type == 'cuda':\n",
    "        model = model.to(torch.bfloat16)\n",
    "        model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1e6:.2f} M parameters\")\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "         qs = input(\"Enter text (q to quit) >>> \")\n",
    "         if qs == \"\":\n",
    "             continue\n",
    "         if qs == 'q':\n",
    "             break\n",
    "  \n",
    "         # we activate chat template only for ORPO model because it was trained with it\n",
    "         if use_orpo:\n",
    "            qs = f\"<s> <|user|>\\n{qs}</s>\\n<s> <|assistant|> \"\n",
    "\n",
    "         x = tokenizer.encode(qs)\n",
    "         x = torch.tensor(x, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "         for ans in range(num_answers):\n",
    "            with torch.no_grad():\n",
    "                y = model.generate(\n",
    "                    x, \n",
    "                    max_new_tokens=256, \n",
    "                    temperature=temp, \n",
    "                    top_k=topk\n",
    "                )\n",
    "\n",
    "            response = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)   \n",
    "\n",
    "            output = model.clean_response(response)\n",
    "\n",
    "            print(\"################## \\n\")\n",
    "            print(f\"### Answer {ans+1}: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b597da-a0c9-418d-89da-97be7da9c19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
