{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f3cb51-cd26-47fb-a8d5-6c69ee2ac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, sys\n",
    "import ipdb # for debugging, variation of pdb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil # detect platform type\n",
    "import requests, zipfile, io\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# these improve performance for Ampere architecture\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Empty GPU Cache Memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa904336-d509-46e2-b762-f9be7049d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files using Python\n"
     ]
    }
   ],
   "source": [
    "files_url = \"https://ideami.com/llm_train\"\n",
    "print(\"Downloading files using Python\")\n",
    "response = requests.get(files_url)\n",
    "# Download and extract\n",
    "#zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2137d03d-7d37-4520-8fee-f0bc824a1edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: you will be using:  mps\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE PARAMETER\n",
    "batch_size = 8 # 8 to 128 and beyond. 8 needs 4GB of GPU, 128 needs 24GB of GPU\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "# each block(layer) includes:\n",
    "# communication: an attention mechanism that learns how the different tokens relate to each other\n",
    "# computation: a layer that provides complex processing for the network\n",
    "n_heads = 7\n",
    "# multi head attention mechanism\n",
    "# the input arrives to the attention mechanism of a block, and it gets divided into a number of Attention Heads which will each process part of that input\n",
    "# After all the heads do their processing, their results get combined together\n",
    "BIAS = True\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "lr = 3e-4 #learning rate (0.0003)\n",
    "dropout = 0.05 # dropout: regulization by randomly turning off a fraction of neurons\n",
    "weight_decay = 0.01 # Weight decay, or L2 regularization, adds a penalty to the loss funciton based on the magnititude of the weights\n",
    "grad_clip = 1.0 # Gradient Clipping, a technique used to prevent exploding gradients by capping the maximum value of gradients during training, ensuring stable and efficient learnin\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "train_iters = 100000\n",
    "eval_interval = 50 # evaluation purpose, so every 50 iteration out of training data to evaluate, the loss supopsed to be higher, but not too much\n",
    "eval_iters = 10 # how many evaluation dataset we will check\n",
    "compile = False # depends on the system, if it works, it's faster and efficient with memory\n",
    "checkpoint_dir = 'models/'\n",
    "checkpoint_fn = 'latest.pt'\n",
    "checkpoint_load_fn = 'latest.pt'\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# MODE\n",
    "inference = False\n",
    "\n",
    "# DEVICE\n",
    "device = torch.device(\"mps\")\n",
    "print (\"device: you will be using: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de9ce81-7879-4005-b521-c456fa20af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGGING\n",
    "\n",
    "#!wandb login --relogin #relogin\n",
    "wandb_log = True\n",
    "wandb_project = \"llm1\"\n",
    "wandb_run_name = \"llm1\"+datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf58b49-277f-4c46-b90b-98688ae10439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "with open('wiki.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[30000:30300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e74f6f53-7dcf-4a9a-a529-e7f40512b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 4096\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZER\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=\"wiki_tokenizer.model\")\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17cac9de-1019-4cee-a03e-3257abe9a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[612, 370, 698, 265, 261, 684]\n",
      "Once upon a time\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "print(encode(\"Once upon a time\"))\n",
    "print(decode(encode(\"Once upon a time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa6cd9dd-6d20-4847-9b56-734c2f75c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"encoded_data.pt\"):\n",
    "    data = torch.load(\"encoded_data.pt\")\n",
    "else:\n",
    "    data = torch.tensor(encode(text), dtyle=torch.long)\n",
    "    torch.save(data, \"encoded_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da190f51-312e-4ed7-b5d5-48054d905b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 59.21 Million | Training: 53.29 Million | Validation: 5.92 Million\n"
     ]
    }
   ],
   "source": [
    "data_size=len(data)\n",
    "spl = int(0.9*data_size)\n",
    "train_data=data[:spl]\n",
    "val_data=data[spl:]\n",
    "\n",
    "print(f'Total data: {data_size/1e6:.2f} Million | Training: {len(train_data)/1e6:.2f} Million | Validation: {len(val_data)/1e6:.2f} Million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d5494d-ed58-4f0f-990a-639a6dbf5b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([1696,  289,  261,  271,  262, 4044,  541, 3137, 4051, 2717],\n",
      "       device='mps:0')\n",
      "tensor([ 289,  261,  271,  262, 4044,  541, 3137, 4051, 2717,  308],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    inds = torch.randint(len(data)-context, (batch_size,)) # a starting point, so it should minus context size\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (Batch Size, Sequence Length), (BS,SL) = (8,512)\n",
    "    y = torch.stack([data[i+1:i+context+1] for i in inds]) # (8, 512) \"+1\" is to see what comes next\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x,y=get_batch(\"train\")\n",
    "print(x.shape, y.shape)\n",
    "print(x[0][:10])\n",
    "print(y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ef6dbd7-4b9f-4230-974d-27091541be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size) # e.g. 4096 x 384\n",
    "        self.positions = nn.Embedding(context, embed_size) # e.g. 512 x 384\n",
    "        #self.blocks = nn.Squential(*[Block(n_heads) for _ in range(n_layers)]) # same as layer, a transformer is made of a number of blocks/layers\n",
    "        # in Python * sign is known as the unpacking operator. It is used to unpack the elements of a list and pass them as individual arguments to a function\n",
    "        self.ln = nn.LayerNorm(embed_size) # Layer normalization. We substract the mean and divide by the standard deviation\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS) # e.g. 384 x 4096 (prediction of all 4096 vocab)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) # Gaussian normal distribution)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input, targets=None):\n",
    "        # BS = Batch size, SL = Sequence or Context Length\n",
    "        loss = None\n",
    "        BS, SL = input.shape # BS x SL e.g. (8,512)\n",
    "        emb = self.embeddings(input) # BS x SL x Embed size (384)\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # SL x 384\n",
    "        x = emb + pos\n",
    "        #x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.final_linear(x) # BS x SL x (vocab size) 4096\n",
    "        # logits are the final predictions of the network for each of the 512 tokens of each of the sequences\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape #BS x SL x 4096\n",
    "            logits = logits.view(BS*SL, VS)\n",
    "            targets = targets.view(BS*SL) \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # Manual Calculation\n",
    "            \n",
    "\n",
    "            # i = - log p(x), when the event is likely happens (high probability), you need a low information\n",
    "            # The entropy is the negative sum of the product of each of the probabilities by the log of that probability\n",
    "            # H(x) = - sum (p(x) * Log p(x))\n",
    "            # Cross Entropy is the negative sum of the probability of each of the elements of the true distribution times the logarithm of the predicted probability for the same element\n",
    "            # H(q,p) = - sum (q(x) * log p(x)) # q = true distribution, p = predicted distribution\n",
    "            # But most of q(x) will be removed, because the probability will be 0 except the right one(1)\n",
    "            # Cross Entropy = - log p(x) (only correct one left)\n",
    "            \n",
    "            counts = logits.exp() # make all the number positive and exaggerate the gap\n",
    "            prob = counts/ counts.sum(-1, keepdim=True)\n",
    "            loss2 = -prob[torch.arange(BS*SL), targets].log().mean()\n",
    "            # targets[3] = 329 | prob[3][329] = 0.014\n",
    "\n",
    "            if(not torch.allclose(loss,loss2)):\n",
    "                print(f\"[Loss Diff] Pytorch:{loss.item()} Manual:{loss2.item()}\")\n",
    "\n",
    "            return logits, loss, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "520c50a3-1d89-4823-b800-3a64c8580ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([2993, 4051,  366, 2221,  973,  393,  304, 4072, 4062, 4059],\n",
      "       device='mps:0')\n",
      "tensor([4051,  366, 2221,  973,  393,  304, 4072, 4062, 4059,  289],\n",
      "       device='mps:0')\n",
      "8.375 8.375\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch(\"train\")\n",
    "print(x.shape, y.shape)\n",
    "print(x[0][:10])\n",
    "print(y[0][:10])\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "logits, loss, loss2 = model(x,y)\n",
    "print(loss.item(), loss2.item()) # check if manual calcualtion and PyTorch Calculation value is the same\n",
    "# this won't be always the same because the manual calcuation is simpler version of the calculation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
