{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f3cb51-cd26-47fb-a8d5-6c69ee2ac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, sys\n",
    "import ipdb # for debugging, variation of pdb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil # detect platform type\n",
    "import requests, zipfile, io\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# these improve performance for Ampere architecture\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Empty GPU Cache Memory\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa904336-d509-46e2-b762-f9be7049d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files using Python\n"
     ]
    }
   ],
   "source": [
    "files_url = \"https://ideami.com/llm_train\"\n",
    "print(\"Downloading files using Python\")\n",
    "response = requests.get(files_url)\n",
    "# Download and extract\n",
    "#zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2137d03d-7d37-4520-8fee-f0bc824a1edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: you will be using:  mps\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE PARAMETER\n",
    "batch_size = 8 # 8 to 128 and beyond. 8 needs 4GB of GPU, 128 needs 24GB of GPU\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "# each block(layer) includes:\n",
    "# communication: an attention mechanism that learns how the different tokens relate to each other\n",
    "# computation: a layer that provides complex processing for the network\n",
    "n_heads = 7\n",
    "# multi head attention mechanism\n",
    "# the input arrives to the attention mechanism of a block, and it gets divided into a number of Attention Heads which will each process part of that input\n",
    "# After all the heads do their processing, their results get combined together\n",
    "BIAS = True\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "lr = 3e-4 #learning rate (0.0003)\n",
    "dropout = 0.05 # dropout: regulization by randomly turning off a fraction of neurons\n",
    "weight_decay = 0.01 # Weight decay, or L2 regularization, adds a penalty to the loss funciton based on the magnititude of the weights\n",
    "grad_clip = 1.0 # Gradient Clipping, a technique used to prevent exploding gradients by capping the maximum value of gradients during training, ensuring stable and efficient learnin\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "train_iters = 100000\n",
    "eval_interval = 50 # evaluation purpose, so every 50 iteration out of training data to evaluate, the loss supopsed to be higher, but not too much\n",
    "eval_iters = 10 # how many evaluation dataset we will check\n",
    "compile = False # depends on the system, if it works, it's faster and efficient with memory\n",
    "\n",
    "load_pretrained = True\n",
    "checkpoint_dir = 'models/'\n",
    "checkpoint_fn = 'latest.pt' # File name for saving a checkpoint\n",
    "checkpoint_load_fn = 'latest.pt' # File name for Loading a checkpoint\n",
    "# lim2.pt is already trained\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# MODE\n",
    "inference = True\n",
    "\n",
    "# DEVICE\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use MPS for acceleration\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "print (\"device: you will be using: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de9ce81-7879-4005-b521-c456fa20af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnickyoon89\u001b[0m (\u001b[33mnickyoon89-miss-to-mrs-box\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nickyoon/Documents/GitHub/LLM_Mastery/basic/wandb/run-20250324_200646-7pykngxa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickyoon89-miss-to-mrs-box/llm9/runs/7pykngxa' target=\"_blank\">llm92025_03_24_20_06_45</a></strong> to <a href='https://wandb.ai/nickyoon89-miss-to-mrs-box/llm9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickyoon89-miss-to-mrs-box/llm9' target=\"_blank\">https://wandb.ai/nickyoon89-miss-to-mrs-box/llm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickyoon89-miss-to-mrs-box/llm9/runs/7pykngxa' target=\"_blank\">https://wandb.ai/nickyoon89-miss-to-mrs-box/llm9/runs/7pykngxa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOGGING\n",
    "\n",
    "#!wandb login --relogin #relogin\n",
    "wandb_log = True\n",
    "wandb_project = \"llm9\" \n",
    "wandb_run_name = \"llm9\"+datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf58b49-277f-4c46-b90b-98688ae10439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "with open('wiki.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[30000:30300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74f6f53-7dcf-4a9a-a529-e7f40512b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 4096\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZER\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=\"wiki_tokenizer.model\")\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17cac9de-1019-4cee-a03e-3257abe9a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[612, 370, 698, 265, 261, 684]\n",
      "Once upon a time\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "print(encode(\"Once upon a time\"))\n",
    "print(decode(encode(\"Once upon a time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa6cd9dd-6d20-4847-9b56-734c2f75c469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoding\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"encoded_data.pt\"):\n",
    "    print(\"Loading encoding\")\n",
    "    data = torch.load(\"encoded_data.pt\")\n",
    "else:\n",
    "    data = torch.tensor(encode(text), dtyle=torch.long)\n",
    "    torch.save(data, \"encoded_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da190f51-312e-4ed7-b5d5-48054d905b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 59.21 Million | Training: 53.29 Million | Validation: 5.92 Million\n"
     ]
    }
   ],
   "source": [
    "data_size=len(data)\n",
    "spl = int(0.9*data_size)\n",
    "train_data=data[:spl]\n",
    "val_data=data[spl:]\n",
    "\n",
    "print(f'Total data: {data_size/1e6:.2f} Million | Training: {len(train_data)/1e6:.2f} Million | Validation: {len(val_data)/1e6:.2f} Million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43d5494d-ed58-4f0f-990a-639a6dbf5b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([ 871,  280, 3195, 4051,  655,  280,  264, 4031, 4062, 4059],\n",
      "       device='mps:0')\n",
      "tensor([ 280, 3195, 4051,  655,  280,  264, 4031, 4062, 4059, 4062],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    inds = torch.randint(len(data)-context, (batch_size,)) # a starting point, so it should minus context size\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (Batch Size, Sequence Length), (BS,SL) = (8,512)\n",
    "    y = torch.stack([data[i+1:i+context+1] for i in inds]) # (8, 512) \"+1\" is to see what comes next\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x,y=get_batch(\"train\")\n",
    "print(x.shape, y.shape)\n",
    "print(x[0][:10])\n",
    "print(y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ef6dbd7-4b9f-4230-974d-27091541be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size) # e.g. 4096 x 384\n",
    "        self.positions = nn.Embedding(context, embed_size) # e.g. 512 x 384\n",
    "        self.blocks = nn.Sequential(*[Block(n_heads) for _ in range(n_layers)]) # same as layer, a transformer is made of a number of blocks/layers\n",
    "        # in Python * sign is known as the unpacking operator. It is used to unpack the elements of a list and pass them as individual arguments to a function\n",
    "        self.ln = nn.LayerNorm(embed_size) # Layer normalization. We substract the mean and divide by the standard deviation\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS) # e.g. 384 x 4096 (prediction of all 4096 vocab)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) # Gaussian normal distribution)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input, targets=None):\n",
    "        \n",
    "        # BS = Batch size, SL = Sequence or Context Length\n",
    "        loss = None\n",
    "        BS, SL = input.shape # BS x SL e.g. (8,512)\n",
    "        emb = self.embeddings(input) # BS x SL x Embed size (384)\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # SL x 384\n",
    "        x = emb + pos\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.final_linear(x) # BS x SL x (vocab size) 4096\n",
    "        # logits are the final predictions of the network for each of the 512 tokens of each of the sequences\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape #BS x SL x 4096\n",
    "            logits = logits.view(BS*SL, VS)\n",
    "            targets = targets.view(BS*SL) \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # Manual Calculation\n",
    "            \n",
    "\n",
    "            # i = - log p(x), when the event is likely happens (high probability), you need a low information\n",
    "            # The entropy is the negative sum of the product of each of the probabilities by the log of that probability\n",
    "            # H(x) = - sum (p(x) * Log p(x))\n",
    "            # Cross Entropy is the negative sum of the probability of each of the elements of the true distribution times the logarithm of the predicted probability for the same element\n",
    "            # H(q,p) = - sum (q(x) * log p(x)) # q = true distribution, p = predicted distribution\n",
    "            # But most of q(x) will be removed, because the probability will be 0 except the right one(1)\n",
    "            # Cross Entropy = - log p(x) (only correct one left)\n",
    "            \n",
    "            counts = logits.exp() # make all the number positive and exaggerate the gap\n",
    "            prob = counts/ counts.sum(-1, keepdim=True) #F.softmax\n",
    "            loss2 = -prob[torch.arange(BS*SL), targets].log().mean()\n",
    "            # targets[3] = 329 | prob[3][329] = 0.014\n",
    "\n",
    "            if(not torch.allclose(loss,loss2)):\n",
    "                print(f\"[Loss Diff] Pytorch:{loss.item()} Manual:{loss2.item()}\")\n",
    "        return logits, loss #, loss2\n",
    "\n",
    "    # Generate a new sample\n",
    "    def generate(self, input, max=500):\n",
    "        for _ in range(max):\n",
    "            input = input[:,-context:] #1, input Length until max of SL)\n",
    "            # taking the last 512 tokens of our input. Every time we generate a new token, it gets added to the input.\n",
    "            # So eventually the input can be longer than 512 tokens. And we can only process 512 tokens in our sequences.\n",
    "            logits, _ = self(input) #(1, input length, 4096), going through the model\n",
    "            logits = logits[:,-1,:] # select only the last prediction (1,4096)\n",
    "            probs = F.softmax(logits, dim=-1) # (1, 4096)\n",
    "            next = torch.multinomial(probs, num_samples=1) # num_samples=1 means just want one value after the token\n",
    "            input = torch.cat((input, next),dim=1)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf4480ac-b5f3-43f1-8fba-3e51aba0343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.ma = Multihead(n_heads, head_size)\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.ma(self.ln1(x))   # Residual connection after multi-head attention\n",
    "        x = x + self.feed_forward(self.ln2(x))  # Residual connection after feed-forward layer\n",
    "        # Residual connections in LLMs add the input directly to the output of certain layers, making training easier and more effective\n",
    "        # They help prevent the vaninshing gradient(gradients become too small gets too small, the training stop happening) problem by allowing gradients to flow more easily throught the network,\n",
    "        # enable the construction of deeper models by combining original and transformed inputs, and improve overall performance and stability.\n",
    "        # This leads to faster convergence during training and better generalization to new data.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bac992b-1630-4ec1-bde2-e39a38f2a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size, 6*embed_size, bias=BIAS),\n",
    "            nn.GELU(), #Non-linear functions\n",
    "            nn.Linear(6*embed_size, embed_size, bias=BIAS),\n",
    "            nn.Dropout(dropout) #deactivate some of the random neurons every training\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x= self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a54c17ae-881a-440e-a4ae-e4efcd1ec67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self,n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias=BIAS) # 378, 384\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # Each head outputs (BS, SL, head_size)\n",
    "        x = self.combine(x) # (BS, SL, 384(embed size))\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "699bd80b-19c7-4794-8d5a-abbac781e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        # the word we're focusing on\n",
    "        self.queries = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        # all the words in the sequence\n",
    "        self.keys = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        # hold the information of these words\n",
    "        self.values = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        # By comparing the query with the keys, the model calculates attention scores, which are then used to weight the values\n",
    "        # The process helps the model decide which words to pay more attention to when making predictions, improving its understanding of context and relationships between words\n",
    "        # (simplified by using word and token interchangeably)\n",
    "\n",
    "        # A buffer is a tensor that is not a model parameter but still needs to saved and restored during model checkpoint\n",
    "        # Buffers are typically used to stored fixed statistics or other intermediate results that should be part of the model's state but do not require gradient update\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context, context))) # mask out all the knowledge about future tokens\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        BS, SL, VS = x.shape\n",
    "        q = self.queries(x) # BS, SL, 54\n",
    "        k = self.keys(x) # BS, SL, 54\n",
    "        v = self.values(x) # BS, SL, 54\n",
    "\n",
    "         # attention weight\n",
    "        attn_w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # BS, SL, 54 @ BS, 54, SL = BS, SL, SL\n",
    "        # k.shape[-1]**-0.5 is to prevent the weight to be too big\n",
    "        # this multification is showing alignment (relationship)\n",
    "        # large positive number(algined, same direction), large negative number(aligned, opposite direction)\n",
    "        # close to zero: vectors are orthogonal(perpendicular) to each other\n",
    "        # The result: the first row will show the compatbility of first token's alignment with all the other tokens in sequence\n",
    "        attn_w = attn_w.masked_fill(self.tril[:SL,:SL]==0, float('-inf')) # change 0 to negative infinity\n",
    "        attn_w = F.softmax(attn_w, dim=-1) # BS, SL, SL\n",
    "        attn_w = self.dropout(attn_w)\n",
    "\n",
    "        x = attn_w @ v # BS, SL, 54\n",
    "        return x\n",
    "        # It is the Dot Product of first token attention scores with second embedding dimension of each of the 512 tokens\n",
    "        # The second embedding dimension of the resulting first token embedding is a weighted sum of the second embedding dimension of the 512 tokens.\n",
    "        # Where the weights are the attention scores between that token and each of the 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae18fe8a-d107-40cb-916d-318b2876ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed: 384 n_heads: 7 head_size: 54\n"
     ]
    }
   ],
   "source": [
    "head_size = embed_size // n_heads\n",
    "print(f\"embed: {embed_size} n_heads: {n_heads} head_size: {head_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "144be4fe-ddef-47f6-9404-d5e1c21ebeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(context, context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d83fa-f91e-40cd-bf86-489a7a046046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL (after doing it, take out the Loss2 from the output of the model)\n",
    "# manual dive for attention\n",
    "'''\n",
    "x,y = get_batch(\"train\")\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "embeddings = nn.Embedding(vocab_size, embed_size).to(device)\n",
    "positions = nn.Embedding(context, embed_size).to(device)\n",
    "queries = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "keys = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "values = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "tril = torch.tril(torch.ones(context,context)).to(device)\n",
    "\n",
    "emb = embeddings(x)\n",
    "pos = positions(torch.arange(context, device=device))\n",
    "x = emb + pos\n",
    "\n",
    "q = queries(x) \n",
    "k = keys(x) \n",
    "v = values(x) \n",
    "print(q.shape, k.shape, v.shape)\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "# torch.set_printoptions(precision=4, threshold=1000, edgeitems=3, linewidth=80, profile='default', sci_mode=True)\n",
    "#print(q[0][0])\n",
    "\n",
    "attn_w = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n",
    "attn_w = attn_w.masked_fill(tril[:context,:context]==0, float('-inf'))\n",
    "attn_w = F.softmax(attn_w, dim=-1)\n",
    "x = attn_w @ v\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ccbc5-8767-4106-8b03-d33203823f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Understand Attention Matrix\n",
    "'''\n",
    "full = q @ k.transpose(-2,-1) # 512 x 54 @ 54 x 512\n",
    "\n",
    "a = q[0][5] # embedding of 54 numbers of of fifth token of the first batch\n",
    "b = k.transpose(-2,-1)[0,:,3]  # embedding of 54 numbers of of third token of the first batch\n",
    "print(a,b)\n",
    "c = torch.dot(a,b)\n",
    "print(c)\n",
    "print(full[0][5][3])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994707da-1f14-4de4-bd0c-ab6af3fd45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Understand the updating of the V content\n",
    "'''\n",
    "print(attn_w.shape, v.shape)\n",
    "\n",
    "print(x[0][7])\n",
    "x = attn_w @ v # 512 x 54\n",
    "\n",
    "attn_scores2 = attn_w[0, 7, :] # Shape [512], 7th token with 512 compatibility with all the 512 tokens\n",
    "# after 7 values, it will be all zero, because we don't care about future token when we consider about 7th token\n",
    "# Initalize a tensor to store the result\n",
    "result = torch.zeros(54)\n",
    "# Compute the dot product for each column in v for the first token in the first batch\n",
    "for i in range(54):\n",
    "    result[i] = torch.dot(attn_scores2, v[0,:,i])\n",
    "print(result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520c50a3-1d89-4823-b800-3a64c8580ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 299,  610,  376, 1181, 1321, 4051,   13, 4064, 4034,  299],\n",
      "       device='mps:0')\n",
      "tensor([ 610,  376, 1181, 1321, 4051,   13, 4064, 4034,  299,  866],\n",
      "       device='mps:0')\n",
      "8.375\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch(\"train\")\n",
    "\n",
    "print(x[0][:10])\n",
    "print(y[0][:10])\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "logits, loss = model(x,y)\n",
    "print(loss.item())\n",
    "\n",
    "# OPTIONAL (after doing it, take out the Loss2 from the output of the model)\n",
    "# comparing loss and loss2(manual simplified calculation)\n",
    "\n",
    "#logits, loss, loss2 = model(x,y)\n",
    "#print(loss.item(), loss2.item()) # check if manual calcualtion and PyTorch Calculation value is the same\n",
    "# this won't be always the same because the manual calcuation is simpler version of the calculation      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3ac42b8-4e48-4b63-ae38-56c3c899b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #decorator, it's an extra feature to a function, in this case, run the function without tracking its operations for gradient calculations\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device)\n",
    "    t1 = t1[None, :] #(1, [size of the ids])\n",
    "    newgen = model.generate(t1, max=64)[0].tolist()\n",
    "    result=decode(newgen)\n",
    "    print(f\"{result}\")\n",
    "\n",
    "# generate_sample(\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d82f0e3-ba7b-4d4c-947e-130e38e1f3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.837954 Million parameters\n"
     ]
    }
   ],
   "source": [
    "# TRAINING SETUP\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "if compile: \n",
    "# requirement to use torch.compile(), this makes ML models run faster and more efficient by converting them into a form that the computer can excute more quickly\n",
    "    print(\"Torch :: Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"Million parameters\")\n",
    "# GPT-3: 175 Billion parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d617525b-39cf-4291-bc52-1ac326c6fb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss Diff] Pytorch:8.375 Manual:8.4375\n",
      "{'train': 8.393750190734863, 'eval': 8.393750190734863}\n"
     ]
    }
   ],
   "source": [
    "# Calculate Loss averages\n",
    "@torch.no_grad()\n",
    "def calculate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','eval']:\n",
    "        l=torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x,y = get_batch(split)\n",
    "            _, loss = model(x,y)\n",
    "            l[i]=loss\n",
    "        out[split]=l.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "l = calculate_loss()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c76c2681-e386-4d65-b76c-b887e554f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the optimizer\n",
    "\n",
    "p_dict = {p_name: p for p_name,p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2]\n",
    "\n",
    "\n",
    "optimizer_groups = [\n",
    "    {'params': weight_decay_p, 'weight_decay': weight_decay},\n",
    "    {'params': no_weight_decay_p, 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=lr, betas=(0.9,0.99))\n",
    "# betas: control the exponential moving averages of the gradient and its square, which are essential components of the Adam and AdamW algorithm\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "# through the training process, change the learning rate(lr), likely slowing down\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "546afe2a-0c8f-4cbb-aa85-91ac6fc3d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint\n",
      "LLM - Loading model\n",
      "Loaded iter 84700 with loss 2.288281202316284\n"
     ]
    }
   ],
   "source": [
    "# Laading Checkpoints\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    print(\"LLM - Loading model\")\n",
    "    checkpoint = torch.load(path, map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    iteration = checkpoint['iteration']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Loaded iter {iteration} with loss {loss}\")\n",
    "    return iteration, loss\n",
    "    \n",
    "if os.path.exists(f\"{checkpoint_dir}/{checkpoint_load_fn}\") and load_pretrained:\n",
    "    print(\"Loading Checkpoint\")\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "150065c5-e6c4-4c7d-9264-db870f69c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit):  Once upon a time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in front of sand, he was captured by the goal geting out of sand and all the other enemies were killed. Once missovae began to reach campo perceived to return across the Alleblad sand flee and honour him be avant-\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit):  Divya is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divya is a friend of Arthur Sumcliffe.\n",
      "\n",
      "It was recommended that he could commit extortion with Palmer in a city near But later.\n",
      "\n",
      "Raymond Park\n",
      "\n",
      "Raymond Rome Hill is an American direct-to-video album directed\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text (q to quit):  q\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "if inference == True:\n",
    "    model.eval()\n",
    "    while True:\n",
    "        qs = input(\"Enter text (q to quit): \")\n",
    "        if qs == \"\":\n",
    "            continue\n",
    "        if qs == \"q\":\n",
    "            break\n",
    "        generate_sample(qs)\n",
    "    #sys.exit() # this is commented out because it doesn't make sense in jupyter notebook structure\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b1391-eb7e-4345-a3ea-303df0c5646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(start_iteration, train_iters)):\n",
    "        xb, yb = get_batch(\"train\")\n",
    "        logits, loss = model(xb,yb)\n",
    "    \n",
    "        # Evaludating loss\n",
    "        if (i % eval_interval == 0 or i == train_iters -1):\n",
    "            l = calculate_loss()\n",
    "            print(f\"\\n{i}: train loss: {l['train']} / val loss: {l['eval']}\")\n",
    "            generate_sample(\"Once upon a time\")\n",
    "    \n",
    "            if l['eval'] < best_val_loss:\n",
    "                best_val_loss = l['eval']\n",
    "                print(\"[CHECKPOINT]: Saving with loss: \", best_val_loss)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_val_loss,\n",
    "                    'iteration': i,\n",
    "                }, checkpoint_dir + checkpoint_fn)\n",
    "    \n",
    "            if wandb_log:\n",
    "                wandb.log({\n",
    "                    \"loss/train\": l['train'],\n",
    "                    \"loss/val\": l['eval'],\n",
    "                    \"lr\": scheduler.get_last_lr()[0],\n",
    "                },\n",
    "                step = i)\n",
    "    \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "    \n",
    "        nn.utils.clip_grad_norm(model.parameters(), max_norm=grad_clip)\n",
    "    \n",
    "        optimizer.step() # Tweak weights\n",
    "        scheduler.step() # Changing the learning rate\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted, Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU memory\n",
    "    # torch.cuda.emty_cache()\n",
    "    print(\"GPU memory released\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# torch.cuda.emty_cache()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce9121-eaf9-4395-8aa9-0896af13202b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
